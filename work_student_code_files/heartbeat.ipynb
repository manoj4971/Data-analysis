{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Noise-resistant heart rate analysis module for Python\n",
    "\n",
    "#Reference:\n",
    "van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2018). Heart Rate Analysis for Human Factors: Development and Validation of an Open Source Toolkit for Noisy Naturalistic Heart Rate Data. In Proceedings of the 6th HUMANIST Conference (pp. 173â€“178)\n",
    "\n",
    "See also:\n",
    "http://www.paulvangent.com/2016/03/15/analyzing-a-discrete-heart-rate-signal-using-python-part-1/\n",
    "http://www.paulvangent.com/2016/03/21/analyzing-a-discrete-heart-rate-signal-using-python-part-2/\n",
    "http://www.paulvangent.com/2016/03/30/analyzing-a-discrete-heart-rate-signal-using-python-part-3/\n",
    "<part 4 to follow after publication>\n",
    "'''\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.signal import butter, filtfilt, welch, periodogram\n",
    "\n",
    "__author__ = \"Paul van Gent\"\n",
    "__version__ = \"Version 0.8.2\"\n",
    "__license__ = \"GNU General Public License V3.0\"\n",
    "\n",
    "measures = {}\n",
    "working_data = {}\n",
    "\n",
    "#Data handling\n",
    "def get_data(filename, delim=',', column_name='None', encoding=None):\n",
    "    '''Loads data from a .CSV or .MAT file into numpy array.\n",
    "\n",
    "    Keyword Arguments:\n",
    "    filename -- absolute or relative path to the file object to read\n",
    "    delim -- the delimiter used if CSV file passed, (default ',')\n",
    "    column_name -- for CSV files with header: specify column that contains the data\n",
    "                   for matlab files it specifies the table name that contains the data\n",
    "                   (default 'None')\n",
    "    '''\n",
    "    file_ext = filename.split('.')[-1]\n",
    "    if file_ext == 'csv' or file_ext == 'txt':\n",
    "        if column_name != 'None':\n",
    "            hrdata = np.genfromtxt(filename, delimiter=delim, names=True, dtype=None, encoding=None)\n",
    "            try:\n",
    "                hrdata = hrdata[column_name]\n",
    "            except Exception as error:\n",
    "                print('\\nError loading column \"%s\" from file \"%s\". \\\n",
    "                Is column name specified correctly?\\n' %(column_name, filename))\n",
    "                print('------\\nError message: ' + str(error) + '\\n------')\n",
    "        elif column_name == 'None':\n",
    "            hrdata = np.genfromtxt(filename, delimiter=delim, dtype=np.float64)\n",
    "        else:\n",
    "            print('\\nError: column name \"%s\" not found in header of \"%s\".\\n'\n",
    "                  %(column_name, filename))\n",
    "    elif file_ext == 'mat':\n",
    "        print('getting matlab file')\n",
    "        import scipy.io\n",
    "        data = scipy.io.loadmat(filename)\n",
    "        if column_name != \"None\":\n",
    "            hrdata = np.array(data[column_name][:, 0], dtype=np.float64)\n",
    "        else:\n",
    "            print(\"\\nError: column name required for Matlab .mat files\\n\\n\")\n",
    "    else:\n",
    "        print('unknown file format')\n",
    "        return None\n",
    "    return hrdata\n",
    "\n",
    "#Preprocessing\n",
    "def scale_data(data):\n",
    "    '''Scales data between 0 and 1024 for analysis\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- numpy array or list to be scaled\n",
    "    '''\n",
    "    range = np.max(data) - np.min(data)\n",
    "    minimum = np.min(data)\n",
    "    data = 1024 * ((data - minimum) / range)\n",
    "    return data\n",
    "\n",
    "def enhance_peaks(hrdata, iterations=2):\n",
    "    '''Attempts to enhance the signal-noise ratio by accentuating the highest peaks\n",
    "    note: denoise first\n",
    "    \n",
    "    Keyword arguments:\n",
    "    hrdata -- numpy array or list containing heart rate data\n",
    "    iterations -- the number of scaling steps to perform (default=3)\n",
    "    '''\n",
    "    scale_data(hrdata)\n",
    "    for i in range(iterations):\n",
    "        hrdata = np.power(hrdata, 2)\n",
    "        hrdata = scale_data(hrdata)\n",
    "    return hrdata    \n",
    "\n",
    "def mark_clipping(hrdata, threshold):\n",
    "    '''function that marks start and end of clipping part\n",
    "    it detects the start and end of clipping segments and returns them\n",
    "    \n",
    "    keyword arguments:\n",
    "    - data: 1d list or numpy array containing heart rate data\n",
    "    - threshold: the threshold for clipping, recommended to\n",
    "                 be a few data points below ADC or sensor max value, \n",
    "                 to compensate for signal noise (default 1020)\n",
    "    \n",
    "    '''\n",
    "    clip_binary = np.where(hrdata > threshold)\n",
    "    clipping_edges = np.where(np.diff(clip_binary) > 1)[1]\n",
    "\n",
    "    clipping_segments = []\n",
    "\n",
    "    for i in range(0, len(clipping_edges)):\n",
    "        if i == 0: #if first clipping segment\n",
    "            clipping_segments.append((clip_binary[0][0], \n",
    "                                      clip_binary[0][clipping_edges[0]]))\n",
    "        elif i == len(clipping_edges):\n",
    "            #append last entry\n",
    "            clipping_segments.append((clip_binary[0][clipping_edges[i]+1],\n",
    "                                      clip_binary[0][-1]))    \n",
    "        else:\n",
    "            clipping_segments.append((clip_binary[0][clipping_edges[i-1] + 1],\n",
    "                                      clip_binary[0][clipping_edges[i]]))\n",
    "\n",
    "    return clipping_segments\n",
    "\n",
    "def interpolate_peaks(hrdata, sample_rate, threshold=1020):\n",
    "    '''function that interpolates peaks between\n",
    "    the clipping segments using cubic spline interpolation.\n",
    "    \n",
    "    It takes the clipping start +/- 100ms to calculate the spline.\n",
    "    \n",
    "    Returns full data array with interpolated segments patched in\n",
    "    \n",
    "    keyword arguments:\n",
    "    data - 1d list or numpy array containing heart rate data\n",
    "    clipping_segments - list containing tuples of start- and \n",
    "                        end-points of clipping segments.\n",
    "    '''\n",
    "    clipping_segments = mark_clipping(hrdata, threshold)\n",
    "    num_datapoints = int(0.1 * sample_rate)\n",
    "    newx = []\n",
    "    newy = []\n",
    "    \n",
    "    for segment in clipping_segments:\n",
    "        if segment[0] < num_datapoints: \n",
    "            #if clipping is present at start of signal, skip.\n",
    "            #We cannot interpolate accurately when there is insufficient data prior to clipping segment.\n",
    "            pass\n",
    "        else: \n",
    "            antecedent = hrdata[segment[0] - num_datapoints : segment[0]]\n",
    "            consequent = hrdata[segment[1] : segment[1] + num_datapoints]\n",
    "            segment_data = np.concatenate((antecedent, consequent))\n",
    "        \n",
    "            interpdata_x = np.concatenate(([x for x in range(segment[0] - num_datapoints, segment[0])],\n",
    "                                            [x for x in range(segment[1], segment[1] + num_datapoints)]))\n",
    "            x_new = np.linspace(segment[0] - num_datapoints,\n",
    "                                segment[1] + num_datapoints,\n",
    "                                ((segment[1] - segment[0]) + (2 * num_datapoints)))\n",
    "        \n",
    "            interp_func = UnivariateSpline(interpdata_x, segment_data, k=3)\n",
    "            interp_data = interp_func(x_new)\n",
    "        \n",
    "            hrdata[segment[0] - num_datapoints :\n",
    "                    segment[1] + num_datapoints] = interp_data\n",
    "       \n",
    "    return hrdata\n",
    "\n",
    "def raw_to_ecg(hrdata, enhancepeaks=False):\n",
    "    '''Flips raw signal with negative mV peaks to normal ECG\n",
    "\n",
    "    Keyword arguments:\n",
    "    hrdata -- numpy array or list containing raw heart rate data\n",
    "    enhancepeaks -- boolean, whether to apply peak accentuation (default False)\n",
    "    '''\n",
    "    hrmean = np.mean(hrdata)\n",
    "    hrdata = (hrmean - hrdata) + hrmean\n",
    "    if enhancepeaks:\n",
    "        hrdata = enhance_peaks(hrdata)\n",
    "    return hrdata\n",
    "\n",
    "def get_samplerate_mstimer(timerdata):\n",
    "    '''Determines sample rate of data from ms-based timer.\n",
    "\n",
    "    Keyword arguments:\n",
    "    timerdata -- array containing values of a timer, in ms\n",
    "    '''\n",
    "    sample_rate = ((len(timerdata) / (timerdata[-1]-timerdata[0]))*1000)\n",
    "    working_data['sample_rate'] = sample_rate\n",
    "    return sample_rate\n",
    "\n",
    "def get_samplerate_datetime(datetimedata, timeformat='%H:%M:%S.%f'):\n",
    "    '''Determines sample rate of data from datetime-based timer.\n",
    "\n",
    "    Keyword arguments:\n",
    "    timerdata -- array containing values of a timer, datetime strings\n",
    "    timeformat -- the format of the datetime-strings in datetimedata\n",
    "    default('%H:%M:%S.f', 24-hour based time including ms: 21:43:12.569)\n",
    "    '''\n",
    "    datetimedata = np.asarray(datetimedata, dtype='str') #cast as str in case of np.bytes type\n",
    "    elapsed = ((datetime.strptime(datetimedata[-1], timeformat) -\n",
    "                datetime.strptime(datetimedata[0], timeformat)).total_seconds())\n",
    "    sample_rate = (len(datetimedata) / elapsed)\n",
    "    working_data['sample_rate'] = sample_rate\n",
    "    return sample_rate\n",
    "\n",
    "def rollwindow(data, windowsize):\n",
    "    '''Returns rolling window of size 'window' over dataset 'data'.\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- 1-dimensional numpy array\n",
    "    window -- window size\n",
    "    '''\n",
    "    shape = data.shape[:-1] + (data.shape[-1] - windowsize + 1, windowsize)\n",
    "    strides = data.strides + (data.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "def rolmean(data, windowsize, sample_rate):\n",
    "    '''Calculates the rolling mean over passed data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- 1-dimensional numpy array or list\n",
    "    windowsize -- the window size to use, in seconds (calculated as windowsize * sample_rate)\n",
    "    sample_rate -- the sample rate of the data set\n",
    "    '''\n",
    "    avg_hr = (np.mean(data))\n",
    "    data_arr = np.array(data)\n",
    "    rol_mean = np.mean(rollwindow(data_arr, int(windowsize*sample_rate)), axis=1)\n",
    "    missing_vals = np.array([avg_hr for i in range(0, int(abs(len(data_arr) - len(rol_mean))/2))])\n",
    "    rol_mean = np.insert(rol_mean, 0, missing_vals)\n",
    "    rol_mean = np.append(rol_mean, missing_vals)\n",
    "\n",
    "    if len(rol_mean) != len(data):\n",
    "        lendiff = len(rol_mean) - len(data)\n",
    "        if lendiff < 0:\n",
    "            rol_mean = np.append(rol_mean, 0)\n",
    "        else:\n",
    "            rol_mean = rol_mean[:-1]            \n",
    "    return rol_mean\n",
    "\n",
    "def butter_lowpass(cutoff, sample_rate, order=2):\n",
    "    '''Defines standard Butterworth lowpass filter.\n",
    "\n",
    "    use 'butter_lowpass_filter' to call the filter.\n",
    "    '''\n",
    "    nyq = 0.5 * sample_rate\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, sample_rate, order):\n",
    "    '''Applies the Butterworth lowpass filter\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- 1-dimensional numpy array or list containing the to be filtered data\n",
    "    cutoff -- the cutoff frequency of the filter\n",
    "    sample_rate -- the sample rate of the data set\n",
    "    order -- the filter order (default 2)\n",
    "    '''\n",
    "    b, a = butter_lowpass(cutoff, sample_rate, order=order)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "def filtersignal(data, cutoff, sample_rate, order):\n",
    "    '''Filters the given signal using a Butterworth lowpass filter.\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- 1-dimensional numpy array or list containing the to be filtered data\n",
    "    cutoff -- the cutoff frequency of the filter\n",
    "    sample_rate -- the sample rate of the data set\n",
    "    order -- the filter order (default 2)\n",
    "    '''\n",
    "    data = np.power(np.array(data), 3)\n",
    "    filtered_data = butter_lowpass_filter(data, cutoff, sample_rate, order)\n",
    "    return filtered_data\n",
    "\n",
    "def MAD(data):\n",
    "    '''function to compute median absolute deviation of data slice\n",
    "       https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
    "    \n",
    "    keyword arguments:\n",
    "    - data: 1-dimensional numpy array containing data\n",
    "    '''\n",
    "    med = np.median(data)\n",
    "    return np.median(np.abs(data - med))\n",
    "\n",
    "def hampelfilt(data, filtsize=6):\n",
    "    '''function to detect outliers based on hampel filter\n",
    "       filter takes datapoint and six surrounding samples.\n",
    "       Detect outliers based on being more than 3std from window mean\n",
    "    \n",
    "    keyword arguments:\n",
    "    - data: 1-dimensional numpy array containing data\n",
    "    - filtsize: the filter size expressed the number of datapoints\n",
    "                taken surrounding the analysed datapoint. a filtsize\n",
    "                of 6 means three datapoints on each side are taken.\n",
    "                total filtersize is thus filtsize + 1 (datapoint evaluated)\n",
    "    '''\n",
    "    output = [x for x in data] #generate second list to prevent overwriting first\n",
    "    onesided_filt = filtsize // 2\n",
    "    #madarray = [0 for x in range(0, onesided_filt)]\n",
    "    for i in range(onesided_filt, len(data) - onesided_filt - 1):\n",
    "        dataslice = output[i - onesided_filt : i + onesided_filt]\n",
    "        mad = MAD(dataslice)\n",
    "        median = np.median(dataslice)\n",
    "        if output[i] > median + (3 * mad):\n",
    "            output[i] = median\n",
    "    return output\n",
    "\n",
    "def hampel_correcter(data, sample_rate, filtsize=6):\n",
    "    '''Returns difference between data and large windowed hampel median filter.\n",
    "       Results in strong noise suppression, but relatively expensive to compute.\n",
    "    '''\n",
    "    return data - hampelfilt(data, filtsize=int(sample_rate))\n",
    "\n",
    "#Peak detection\n",
    "def detect_peaks(hrdata, rol_mean, ma_perc, sample_rate, update_dict=True):\n",
    "    '''Detects heartrate peaks in the given dataset.\n",
    "\n",
    "    Keyword arguments:\n",
    "    hr data -- 1-dimensional numpy array or list containing the heart rate data\n",
    "    rol_mean -- 1-dimensional numpy array containing the rolling mean of the heart rate signal\n",
    "    ma_perc -- the percentage with which to raise the rolling mean,\n",
    "    used for fitting detection solutions to data\n",
    "    sample_rate -- the sample rate of the data set\n",
    "    update_dict -- whether to update the peak information in the module's data structure\n",
    "                   Setting this to False (default True) allows peak function to be re-used for\n",
    "                   example by the breath analysis module.\n",
    "    '''\n",
    "    rmean = np.array(rol_mean)\n",
    "    rol_mean = rmean + ((rmean / 100) * ma_perc)\n",
    "    peaksx = np.where((hrdata > rol_mean))[0]\n",
    "    peaksy = hrdata[np.where((hrdata > rol_mean))[0]]\n",
    "    peakedges = np.concatenate((np.array([0]),\n",
    "                                (np.where(np.diff(peaksx) > 1)[0]),\n",
    "                                np.array([len(peaksx)])))\n",
    "    peaklist = []\n",
    "\n",
    "    for i in range(0, len(peakedges)-1):\n",
    "        try:\n",
    "            y_values = peaksy[peakedges[i]:peakedges[i+1]].tolist()\n",
    "            peaklist.append(peaksx[peakedges[i] + y_values.index(max(y_values))])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if update_dict:\n",
    "        working_data['peaklist'] = peaklist\n",
    "        working_data['ybeat'] = [hrdata[x] for x in peaklist]\n",
    "        working_data['rolmean'] = rol_mean\n",
    "        calc_rr(sample_rate)\n",
    "        if len(working_data['RR_list']):\n",
    "            working_data['rrsd'] = np.std(working_data['RR_list'])\n",
    "        else:\n",
    "            working_data['rrsd'] = np.inf\n",
    "    else:\n",
    "        return peaklist\n",
    "\n",
    "def fit_peaks(hrdata, rol_mean, sample_rate, bpmmin=40, bpmmax=180):\n",
    "    '''Runs fitting with varying peak detection thresholds given a heart rate signal.\n",
    "       Results in relatively noise-robust, temporally accuract peak detection, as no\n",
    "       non-linear transformations are involved that might shift peak positions.\n",
    "\n",
    "    Keyword arguments:\n",
    "    hrdata - 1-dimensional numpy array or list containing the heart rate data\n",
    "    rol_mean -- 1-dimensional numpy array containing the rolling mean of the heart rate signal\n",
    "    sample_rate -- the sample rate of the data set\n",
    "    bpmmin -- minimum value of bpm to see as likely (default 40)\n",
    "    bpmmax -- maximum value of bpm to see as likely (default 180)\n",
    "    '''\n",
    "    ma_perc_list = [5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 150, 200, 300]\n",
    "    rrsd = []\n",
    "    valid_ma = []\n",
    "    for ma_perc in ma_perc_list:\n",
    "        detect_peaks(hrdata, rol_mean, ma_perc, sample_rate)\n",
    "        bpm = ((len(working_data['peaklist'])/(len(hrdata)/sample_rate))*60)\n",
    "        rrsd.append([working_data['rrsd'], bpm, ma_perc])\n",
    "\n",
    "    for _rrsd, _bpm, _ma_perc in rrsd:\n",
    "        if (_rrsd > 0.1) and ((bpmmin <= _bpm <= bpmmax)):\n",
    "            valid_ma.append([_rrsd, _ma_perc])\n",
    "\n",
    "\n",
    "    working_data['best'] = min(valid_ma, key=lambda t: t[0])[1]\n",
    "    detect_peaks(hrdata, rol_mean, min(valid_ma, key=lambda t: t[0])[1], sample_rate)\n",
    "\n",
    "def check_peaks(reject_segmentwise=False):\n",
    "    '''Determines the best fit for peak detection variations run by fit_peaks().'''\n",
    "    rr_arr = np.array(working_data['RR_list'])\n",
    "    peaklist = np.array(working_data['peaklist'])\n",
    "    ybeat = np.array(working_data['ybeat'])\n",
    "    mean_rr = np.mean(rr_arr)\n",
    "    upper_threshold = mean_rr + 300 if (0.3 * mean_rr) <= 300 else mean_rr + (0.3 * mean_rr)\n",
    "    lower_threshold = mean_rr - 300 if (0.3 * mean_rr) <= 300 else mean_rr - (0.3 * mean_rr)\n",
    "\n",
    "    peaklist_cor = peaklist[np.where((rr_arr > lower_threshold) &\n",
    "                                     (rr_arr < upper_threshold))[0]+1]\n",
    "    working_data['peaklist_cor'] = np.insert(peaklist_cor, 0, peaklist[0])\n",
    "    working_data['removed_beats'] = peaklist[np.where((rr_arr <= lower_threshold) |\n",
    "                                                      (rr_arr >= upper_threshold))[0]+1]\n",
    "    working_data['removed_beats_y'] = ybeat[np.where((rr_arr <= lower_threshold) |\n",
    "                                                     (rr_arr >= upper_threshold))[0]+1]\n",
    "    working_data['binary_peaklist'] = [0 if x in working_data['removed_beats'] \n",
    "                                       else 1 for x in working_data['peaklist']]\n",
    "    if(reject_segmentwise): \n",
    "        check_binary_quality(working_data['binary_peaklist'])\n",
    "    update_rr()\n",
    "\n",
    "def check_binary_quality(binary_peaklist, maxrejects=3):\n",
    "    '''Checks signal in chunks of 10 beats. \n",
    "    Zeros out chunk if number of rejected peaks > maxrejects.\n",
    "    Also marks rejected segment coordinates in tuples (x[0], x[1] in working_data['rejected_segments']\n",
    "    \n",
    "    Keyword arugments:\n",
    "    binary_peaklist: list with 0 and 1 corresponding to r-peak accept/reject decisions\n",
    "    maxrejects: int, maximum number of rejected peaks per 10-beat window (default 3)\n",
    "    '''\n",
    "    idx = 0\n",
    "    peaklist = working_data['peaklist']\n",
    "    working_data['rejected_segments'] = []\n",
    "    for i in range(int(len(binary_peaklist) / 10)):\n",
    "        if np.bincount(binary_peaklist[idx:idx + 10])[0] > maxrejects:\n",
    "            binary_peaklist[idx:idx + 10] = [0 for i in range(len(binary_peaklist[idx:idx+10]))]\n",
    "            if idx + 10 < len(peaklist): \n",
    "                working_data['rejected_segments'].append((peaklist[idx], peaklist[idx + 10]))\n",
    "            else:\n",
    "                working_data['rejected_segments'].append((peaklist[idx], peaklist[-1]))\n",
    "        idx += 10\n",
    "\n",
    "#Calculating all measures\n",
    "def calc_rr(sample_rate):\n",
    "    '''Calculates the R-R (peak-peak) data required for further analysis.\n",
    "\n",
    "    Uses calculated measures stored in the working_data{} dict to calculate\n",
    "    all required peak-peak datasets. Stores results in the working_data{} dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    sample_rate -- the sample rate of the data set\n",
    "    '''\n",
    "    peaklist = np.array(working_data['peaklist'])\n",
    "\n",
    "    #delete first peak if within first 150ms (signal might start mid-beat after peak)\n",
    "    if len(peaklist) > 0:\n",
    "        if peaklist[0] <= ((sample_rate / 1000.0) * 150):\n",
    "            peaklist = np.delete(peaklist, 0)\n",
    "            working_data['peaklist'] = peaklist\n",
    "            working_data['ybeat'] = np.delete(working_data['ybeat'], 0)\n",
    "\n",
    "    rr_list = (np.diff(peaklist) / sample_rate) * 1000.0\n",
    "    rr_diff = np.abs(np.diff(rr_list))\n",
    "    rr_sqdiff = np.power(rr_diff, 2)\n",
    "    working_data['RR_list'] = rr_list\n",
    "    working_data['RR_diff'] = rr_diff\n",
    "    working_data['RR_sqdiff'] = rr_sqdiff\n",
    "\n",
    "def update_rr():\n",
    "    '''Updates RR differences and RR squared differences based on corrected RR list\n",
    "\n",
    "    Uses information about rejected peaks to update RR_list_cor, and RR_diff, RR_sqdiff\n",
    "    in the working_data{} dict.\n",
    "    '''\n",
    "    rr_source = working_data['RR_list']\n",
    "    b_peaks = working_data['binary_peaklist']\n",
    "    rr_list = [rr_source[i] for i in range(len(rr_source)) if b_peaks[i] + b_peaks[i+1] == 2]\n",
    "    rr_mask = [0 if (b_peaks[i] + b_peaks[i+1] == 2) else 1 for i in range(len(rr_source))]\n",
    "    rr_masked = np.ma.array(rr_source, mask=rr_mask)\n",
    "    rr_diff = np.abs(np.diff(rr_masked))\n",
    "    rr_diff = rr_diff[~rr_diff.mask]\n",
    "    rr_sqdiff = np.power(rr_diff, 2)\n",
    "    \n",
    "    working_data['RR_masklist'] = rr_mask\n",
    "    working_data['RR_list_cor'] = rr_list\n",
    "    working_data['RR_diff'] = rr_diff\n",
    "    working_data['RR_sqdiff'] = rr_sqdiff\n",
    "\n",
    "def calc_ts_measures():\n",
    "    '''Calculates the time-series measurements.\n",
    "\n",
    "    Uses calculated measures stored in the working_data{} dict to calculate\n",
    "    the time-series measurements of the heart rate signal.\n",
    "    Stores results in the measures{} dict object.\n",
    "    '''\n",
    "    rr_list = working_data['RR_list_cor']\n",
    "    rr_diff = working_data['RR_diff']\n",
    "    rr_sqdiff = working_data['RR_sqdiff']\n",
    "    \n",
    "    measures['bpm'] = 60000 / np.mean(rr_list)\n",
    "    measures['ibi'] = np.mean(rr_list)\n",
    "    measures['sdnn'] = np.std(rr_list)\n",
    "    measures['sdsd'] = np.std(rr_diff)\n",
    "    measures['rmssd'] = np.sqrt(np.mean(rr_sqdiff))\n",
    "    nn20 = [x for x in rr_diff if x > 20]\n",
    "    nn50 = [x for x in rr_diff if x > 50]\n",
    "    measures['nn20'] = nn20\n",
    "    measures['nn50'] = nn50\n",
    "    measures['pnn20'] = float(len(nn20)) / float(len(rr_diff))\n",
    "    measures['pnn50'] = float(len(nn50)) / float(len(rr_diff))\n",
    "    measures['hr_mad'] = MAD(rr_list)\n",
    "\n",
    "def calc_fd_measures(hrdata, sample_rate, method='welch'):\n",
    "    '''Calculates the frequency-domain measurements.\n",
    "\n",
    "    Uses calculated measures stored in the working_data{} dict to calculate\n",
    "    the frequency-domain measurements of the heart rate signal.\n",
    "    Stores results in the measures{} dict object.\n",
    "    '''\n",
    "    rr_list = working_data['RR_list_cor']\n",
    "    rr_x = []\n",
    "    pointer = 0\n",
    "    for x in rr_list:\n",
    "        pointer += x\n",
    "        rr_x.append(pointer)\n",
    "    rr_x_new = np.linspace(rr_x[0], rr_x[-1], rr_x[-1])\n",
    "    interpolated_func = UnivariateSpline(rr_x, rr_list, k=3)\n",
    "    \n",
    "    if method=='fft':\n",
    "        datalen = len(rr_x_new)\n",
    "        frq = np.fft.fftfreq(datalen, d=((1/1000.0)))\n",
    "        frq = frq[range(int(datalen/2))]\n",
    "        Y = np.fft.fft(interpolated_func(rr_x_new))/datalen\n",
    "        Y = Y[range(int(datalen/2))]\n",
    "        psd = np.power(Y, 2)\n",
    "    elif method=='periodogram':\n",
    "        frq, psd = periodogram(interpolated_func(rr_x_new), fs=1000.0)\n",
    "    elif method=='welch':\n",
    "        frq, psd = welch(interpolated_func(rr_x_new), fs=1000.0, nperseg=100000)\n",
    "    else:\n",
    "        print(\"specified method incorrect, use 'fft', 'periodogram' or 'welch'\")\n",
    "        raise SystemExit(0)\n",
    "    \n",
    "    measures['lf'] = np.trapz(abs(psd[(frq >= 0.04) & (frq <= 0.15)]))\n",
    "    measures['hf'] = np.trapz(abs(psd[(frq >= 0.16) & (frq <= 0.5)]))\n",
    "    measures['lf/hf'] = measures['lf'] / measures['hf']\n",
    "    measures['interp_rr_function'] = interpolated_func\n",
    "    measures['interp_rr_linspace'] = (rr_x[0], rr_x[-1], rr_x[-1])\n",
    "\n",
    "def calc_breathing(sample_rate):\n",
    "    '''function to estimate breathing rate from heart rate signal.\n",
    "    \n",
    "    Upsamples the list of detected rr_intervals by interpolation\n",
    "    then tries to extract breathing peaks in the signal.\n",
    "\n",
    "    keyword arguments:\n",
    "    sample_rate -- sample rate of the heart rate signal\n",
    "    '''\n",
    "    rrlist = working_data['RR_list_cor']\n",
    "    x = np.linspace(0, len(rrlist), len(rrlist))\n",
    "    x_new = np.linspace(0, len(rrlist), len(rrlist)*10)\n",
    "    interp = UnivariateSpline(x, rrlist, k=3)\n",
    "    breathing = interp(x_new)\n",
    "    breathing_rolmean = rolmean(breathing, 0.75, 100.0)\n",
    "    peaks = detect_peaks(breathing, breathing_rolmean, 1, sample_rate, update_dict=False)\n",
    "    \n",
    "    if len(peaks) > 1:\n",
    "        signaltime = len(working_data['hr']) / sample_rate\n",
    "        measures['breathingrate'] = len(peaks) / signaltime\n",
    "    else:\n",
    "        measures['breathingrate'] = np.nan\n",
    "\n",
    "#Plotting it\n",
    "def plotter(show=True, title='Heart Rate Signal Peak Detection', reject_segmentwise=False):\n",
    "    '''Plots the analysis results.\n",
    "\n",
    "    Uses calculated measures and data stored in the working_data{} and measures{}\n",
    "    dict objects to visualise the fitted peak detection solution.\n",
    "\n",
    "    Keyword arguments:\n",
    "    show -- whether to display the plot (True) or return a plot object (False) (default True)\n",
    "    title -- the title used in the plot\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    peaklist = working_data['peaklist']\n",
    "    ybeat = working_data['ybeat']\n",
    "    rejectedpeaks = working_data['removed_beats']\n",
    "    rejectedpeaks_y = working_data['removed_beats_y']\n",
    "    plt.title(title)\n",
    "    plt.plot(working_data['hr'], alpha=0.5, color='blue', label='heart rate signal')\n",
    "    plt.scatter(peaklist, ybeat, color='green', label='BPM:%.2f' %(measures['bpm']))\n",
    "    plt.scatter(rejectedpeaks, rejectedpeaks_y, color='red', label='rejected peaks')\n",
    "    if(reject_segmentwise):\n",
    "        for segment in working_data['rejected_segments']:\n",
    "            plt.axvspan(segment[0], segment[1], facecolor='red', alpha=0.5)\n",
    "    plt.legend(loc=4, framealpha=0.6)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return plt\n",
    "\n",
    "#Wrapper function\n",
    "def process(hrdata, sample_rate, windowsize=0.75, report_time=False, \n",
    "            calc_freq=False, freq_method='welch', interp_clipping=True, clipping_scale=False,\n",
    "            interp_threshold=1020, hampel_correct=False, bpmmin=40, bpmmax=180,\n",
    "            reject_segmentwise=False):\n",
    "    '''Processed the passed heart rate data. Returns measures{} dict containing results.\n",
    "\n",
    "    Keyword arguments:\n",
    "    hrdata -- 1-dimensional numpy array or list containing heart rate data\n",
    "    sample_rate -- the sample rate of the heart rate data\n",
    "    windowsize -- the window size to use, in seconds (calculated as windowsize * sample_rate)\n",
    "    report_time -- whether to report total processing time of algorithm (default True)\n",
    "    calc_freq -- whether to compute time-series measurements (default False)\n",
    "    interp_clipping -- whether to detect and interpolate clipping segments of the signal \n",
    "                       (default True)\n",
    "    intep_threshold -- threshold to use to detect clipping segments. Recommended to be a few\n",
    "                       datapoints below the sensor or ADC's maximum value (to account for\n",
    "                       slight data line noise). Default 1020, 4 below max of 1024 for 10-bit ADC\n",
    "    hampel_correct -- whether to reduce noisy segments using large median filter. Disabled by\n",
    "                      default due to computational complexity, and generally it is not necessary\n",
    "    bpmmin -- minimum value to see as likely for BPM when fitting peaks\n",
    "    bpmmax -- maximum value to see as likely for BPM when fitting peaks\n",
    "    '''\n",
    "    t1 = time.clock()\n",
    "\n",
    "    if interp_clipping:\n",
    "        if clipping_scale:\n",
    "            hrdata = scale_data(hrdata)\n",
    "        hrdata = interpolate_peaks(hrdata, sample_rate, threshold=interp_threshold)\n",
    "\n",
    "    if hampel_correct:\n",
    "        hrdata = enhance_peaks(hrdata)\n",
    "        hrdata = hampel_correcter(hrdata, sample_rate, filtsize=sample_rate)\n",
    "\n",
    "    working_data['hr'] = hrdata\n",
    "    rol_mean = rolmean(hrdata, windowsize, sample_rate)\n",
    "    fit_peaks(hrdata, rol_mean, sample_rate)\n",
    "    calc_rr(sample_rate)\n",
    "    check_peaks(reject_segmentwise)\n",
    "    calc_ts_measures()\n",
    "    calc_breathing(sample_rate)\n",
    "    if calc_freq:\n",
    "        calc_fd_measures(hrdata, sample_rate)\n",
    "    if report_time:\n",
    "        print('\\nFinished in %.8s sec' %(time.clock()-t1))\n",
    "    return measures\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hrdata = get_data('data.csv')\n",
    "    fs = 100.0\n",
    "\n",
    "    #hrdata = get_data('data2.csv', column_name = 'hr')\n",
    "    #fs = get_samplerate_mstimer(get_data('data2.csv', column_name='timer'))\n",
    "\n",
    "    measures = process(hrdata, fs, report_time=True, calc_freq =True, interp_clipping=True, hampel_correct=False)\n",
    "\n",
    "    for m in measures.keys():\n",
    "        print(m + ': ' + str(measures[m]))\n",
    "    \n",
    "    plotter()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
